"""
Medication RAG Core Module
- ìœ„ê³ ë¹„/ë§ˆìš´ìë¡œ ì „ë¬¸ ì˜ì•½í’ˆ ì •ë³´ RAG
- ì‘ê¸‰ ìƒí™© ê°ì§€ ë° ì˜ë£Œ ì•ˆì „ ê°€ë“œë ˆì¼ í¬í•¨
"""

import os
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# .env.local ë¡œë“œ
BASE_DIR = Path(__file__).parent
PROJECT_ROOT = BASE_DIR.parent
load_dotenv(PROJECT_ROOT / ".env.local")

from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI

# Paths
STORAGE_DIR = BASE_DIR / "storage"

# Retrieval settings
SIMILARITY_TOP_K = 4

# ì‘ê¸‰ ìƒí™© í‚¤ì›Œë“œ (í•œêµ­ì–´ + ì˜ì–´)
EMERGENCY_KEYWORDS = [
    # ì‹¬ê°í•œ ë¶€ì‘ìš©
    "í‰í†µ", "ê°€ìŠ´í†µì¦", "í˜¸í¡ê³¤ë€", "ìˆ¨ì„ ëª»", "ì‹¬í•œ ë³µí†µ",
    "ì˜ì‹ì €í•˜", "ì˜ì‹ë¶ˆëª…", "ê¸°ì ˆ", "ê²½ë ¨", "ë°œì‘",
    "ì•„ë‚˜í•„ë½ì‹œìŠ¤", "ì‡¼í¬", "ì‹¬í•œ ì•Œë ˆë¥´ê¸°",
    "ê°‘ìƒì„ ì•”", "ì·Œì¥ì—¼", "ì‹¬í•œ êµ¬í† ", "íƒˆìˆ˜",
    # ì˜ì–´
    "chest pain", "difficulty breathing", "unconscious",
    "severe pain", "anaphylaxis", "pancreatitis",
]

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë¹„ë§Œ ì¹˜ë£Œì œ(ìœ„ê³ ë¹„, ë§ˆìš´ìë¡œ) ì „ë¬¸ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

## ì—­í• 
- ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ê³µì‹ í—ˆê°€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
- ì‚¬ìš©ìì˜ ê±´ê°• ë°ì´í„°(ì²´ì¤‘, ì¹¼ë¡œë¦¬, ë³µìš© ê¸°ë¡)ë¥¼ ê³ ë ¤í•œ ë§ì¶¤ ì¡°ì–¸
- ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…

## ê·œì¹™
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: í—ˆê°€ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€
2. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ ì‹œ "í—ˆê°€ì‚¬í•­ì— ë”°ë¥´ë©´~" í˜•ì‹ ì‚¬ìš©
3. **í™•ì •ì  í‘œí˜„ ê¸ˆì§€**: "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤" ì‚¬ìš©
4. **ì‘ê¸‰ ìƒí™© ìš°ì„ **: ì‹¬ê°í•œ ë¶€ì‘ìš© ì¦ìƒ ì‹œ ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸ ì•ˆë‚´
5. **ì˜ë£Œ ë©´ì±…**: ê°œì¸ ë§ì¶¤ ì˜ë£Œ ì¡°ì–¸ì´ ì•„ë‹˜ì„ ëª…ì‹œ

## ë‹µë³€ í˜•ì‹
- 2-3ë¬¸ë‹¨ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
- í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì €, ë¶€ê°€ ì„¤ëª…ì€ ë‚˜ì¤‘ì—
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸ’Š ğŸ“‹ âš ï¸ âœ…)

*ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì¹˜ë£ŒëŠ” ë‹´ë‹¹ ì˜ì‚¬ì™€ ìƒë‹´í•˜ì„¸ìš”.*
"""

EMERGENCY_RESPONSE = """
ğŸš¨ **ì‘ê¸‰ ìƒí™© ì•ˆë‚´**

ë§ì”€í•˜ì‹  ì¦ìƒì´ ì‹¬ê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ì¦‰ì‹œ ë‹¤ìŒ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”:**

1. **119ì— ì „í™”**í•˜ê±°ë‚˜ **ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤**ì„ ë°©ë¬¸í•˜ì„¸ìš”
2. ë³µìš© ì¤‘ì¸ ì•½ë¬¼ ì •ë³´ë¥¼ ì˜ë£Œì§„ì—ê²Œ ì•Œë ¤ì£¼ì„¸ìš”
3. ì¦ìƒì´ ì‹œì‘ëœ ì‹œê°„ì„ ê¸°ì–µí•´ë‘ì„¸ìš”

---

"""


class MedicationRAG:
    """ìœ„ê³ ë¹„/ë§ˆìš´ìë¡œ ì „ë¬¸ RAG ì‹œìŠ¤í…œ"""

    def __init__(self, storage_dir: Optional[Path] = None):
        self.storage_dir = storage_dir or STORAGE_DIR
        self.index = None
        self.query_engine = None
        self._initialized = False

    def initialize(self):
        """ì¸ë±ìŠ¤ ë¡œë“œ (lazy initialization)"""
        if self._initialized:
            return

        if not self.storage_dir.exists():
            raise FileNotFoundError(
                f"Storage not found: {self.storage_dir}\n"
                "Run 'python fetch_documents.py' and 'python build_index.py' first."
            )

        print("[RAG] Loading embedding model...")
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
        )

        print("[RAG] Configuring LLM...")
        Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0.2)

        print("[RAG] Loading index...")
        storage_context = StorageContext.from_defaults(
            persist_dir=str(self.storage_dir)
        )
        self.index = load_index_from_storage(storage_context)

        self.query_engine = self.index.as_query_engine(
            similarity_top_k=SIMILARITY_TOP_K,
            system_prompt=SYSTEM_PROMPT,
        )

        self._initialized = True
        print("[RAG] Ready!")

    def _detect_emergency(self, query: str) -> bool:
        """ì‘ê¸‰ í‚¤ì›Œë“œ ê°ì§€"""
        query_lower = query.lower()
        return any(kw in query_lower for kw in EMERGENCY_KEYWORDS)

    def _format_sources(self, response) -> str:
        """ì¶œì²˜ í¬ë§·íŒ…"""
        if not response.source_nodes:
            return ""

        sources = []
        seen = set()

        for node in response.source_nodes:
            source = node.metadata.get("file_name", "ì•Œ ìˆ˜ ì—†ìŒ")
            # íŒŒì¼ëª…ì—ì„œ ì•½ë¬¼ëª… ì¶”ì¶œ
            if source.startswith("wegovy"):
                source = "ìœ„ê³ ë¹„ í—ˆê°€ì‚¬í•­"
            elif source.startswith("mounjaro"):
                source = "ë§ˆìš´ìë¡œ í—ˆê°€ì‚¬í•­"

            if source not in seen:
                seen.add(source)
                sources.append(source)

        return ", ".join(sources)

    def ask(self, query: str, user_context: str = "", use_rag: bool = True) -> str:
        """
        ì§ˆë¬¸ì— ë‹µë³€

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            user_context: ê±´ê°• ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
            use_rag: RAG ì‚¬ìš© ì—¬ë¶€ (Falseë©´ LLMë§Œ ì‚¬ìš©)

        Returns:
            í¬ë§·ëœ ì‘ë‹µ
        """
        self.initialize()

        result_parts = []

        # ì‘ê¸‰ ìƒí™© ì²´í¬
        if self._detect_emergency(query):
            result_parts.append(EMERGENCY_RESPONSE)

        # ì „ì²´ ì¿¼ë¦¬ êµ¬ì„±
        full_query = query
        if user_context:
            full_query = f"""## ì‚¬ìš©ì ê±´ê°• ì •ë³´
{user_context}

## ì§ˆë¬¸
{query}"""

        if use_rag:
            # RAG ì‚¬ìš©
            response = self.query_engine.query(full_query)
            result_parts.append(str(response))

            # ì¶œì²˜ ì¶”ê°€
            sources = self._format_sources(response)
            if sources:
                result_parts.append(f"\n\nğŸ“‹ **ì°¸ê³ **: {sources}")
        else:
            # LLMë§Œ ì‚¬ìš© (í† í° ì ˆì•½)
            from openai import OpenAI as OpenAIClient

            client = OpenAIClient(api_key=os.getenv("OPENAI_API_KEY"))

            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": full_query},
                ],
                max_tokens=500,
                temperature=0.3,
            )

            result_parts.append(completion.choices[0].message.content)

        return "".join(result_parts)

    def is_emergency(self, query: str) -> bool:
        """ì‘ê¸‰ ìƒí™© ì—¬ë¶€ ë°˜í™˜"""
        return self._detect_emergency(query)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_rag_instance: Optional[MedicationRAG] = None


def get_rag() -> MedicationRAG:
    """RAG ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = MedicationRAG()
    return _rag_instance


def ask(query: str, user_context: str = "") -> str:
    """í¸ì˜ í•¨ìˆ˜"""
    return get_rag().ask(query, user_context)
