"""
RAG Core Module for Medical Document Q&A
- Loads persisted index from storage/
- Provides ask() function with medical safety rules
- Includes emergency detection and appropriate disclaimers
"""

from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from parent .env.local
BASE_DIR = Path(__file__).parent
PROJECT_ROOT = BASE_DIR.parent
load_dotenv(PROJECT_ROOT / ".env.local")

from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI

# Paths
STORAGE_DIR = BASE_DIR / "storage"

# Retrieval settings
SIMILARITY_TOP_K = 5

# Emergency keywords (Korean + English)
EMERGENCY_KEYWORDS = [
    # Korean
    "í‰í†µ", "ê°€ìŠ´í†µì¦", "í˜¸í¡ê³¤ëž€", "ìˆ¨ì„ ëª»", "ì˜ì‹ì €í•˜", "ì˜ì‹ë¶ˆëª…",
    "ê¸°ì ˆ", "ì‹¬í•œ ì¶œí˜ˆ", "ëŒ€ëŸ‰ ì¶œí˜ˆ", "ê³¼ë‹¤ì¶œí˜ˆ", "ìžì‚´", "ìží•´",
    "ì‹¬ìž¥ë§ˆë¹„", "ë‡Œì¡¸ì¤‘", "ê²½ë ¨", "ë°œìž‘", "ì•„ë‚˜í•„ë½ì‹œìŠ¤", "ì‡¼í¬",
    # English
    "chest pain", "difficulty breathing", "unconscious", "severe bleeding",
    "heart attack", "stroke", "seizure", "anaphylaxis", "suicidal",
]

# Medical safety system prompt
MEDICAL_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì˜í•™ ë¬¸ì„œ ê¸°ë°˜ ì •ë³´ ì œê³µ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤.

## í•µì‹¬ ê·œì¹™

1. **ê·¼ê±° ê¸°ë°˜ ì‘ë‹µë§Œ ì œê³µ**
   - ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ì •ë³´ê°€ ì œê³µëœ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
   - ì¶”ì¸¡, ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ ì ˆëŒ€ ê¸ˆì§€

2. **ì¶œì²˜ ëª…ì‹œ**
   - ëª¨ë“  ë‹µë³€ì— ê·¼ê±° ë¬¸ì„œ/ì„¹ì…˜ ìš”ì•½ í¬í•¨
   - ê°€ëŠ¥í•œ ê²½ìš° ë¬¸ì„œëª… ì–¸ê¸‰

3. **ì˜ë£Œ í‘œí˜„ ì œí•œ**
   - "~ìž…ë‹ˆë‹¤", "~í•´ì•¼ í•©ë‹ˆë‹¤" ê°™ì€ í™•ì •ì  ì§„ë‹¨/ì²˜ë°© í‘œí˜„ ê¸ˆì§€
   - "~í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤", "~ë¥¼ ê³ ë ¤í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤", "ë¬¸ì„œì— ë”°ë¥´ë©´~" ì‚¬ìš©

4. **ì‘ê¸‰ ìƒí™© ëŒ€ì‘**
   - ì‘ê¸‰ ì‹ í˜¸ ê°ì§€ ì‹œ ì¦‰ì‹œ 119/ì‘ê¸‰ì‹¤ ì•ˆë‚´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì œê³µ
   - ì‘ê¸‰ ì‹ í˜¸: í‰í†µ, í˜¸í¡ê³¤ëž€, ì˜ì‹ì €í•˜, ì‹¬í•œ ì¶œí˜ˆ, ì‹¬í•œ ì•Œë ˆë¥´ê¸° ë°˜ì‘ ë“±

5. **ê³ ì§€ ì˜ë¬´**
   - ëª¨ë“  ë‹µë³€ ëì— ë‹¤ìŒ ë¬¸êµ¬ í¬í•¨:
   "ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì§„ë‹¨ ë° ì¹˜ë£ŒëŠ” ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”."

## ë‹µë³€ í˜•ì‹

[ë‹µë³€ ë‚´ìš©]

**ê·¼ê±° ë¬¸ì„œ:** [ì¶œì²˜ ì •ë³´]

---
*ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì§„ë‹¨ ë° ì¹˜ë£ŒëŠ” ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.*
"""

EMERGENCY_RESPONSE_PREFIX = """
ðŸš¨ **ì‘ê¸‰ ìƒí™© ì•ˆë‚´**

ì¦ìƒì´ ì‹¬ê°í•´ ë³´ìž…ë‹ˆë‹¤. **ì¦‰ì‹œ ë‹¤ìŒ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”:**

1. **119ì— ì „í™”**í•˜ê±°ë‚˜ **ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤**ì„ ë°©ë¬¸í•˜ì„¸ìš”
2. ì£¼ë³€ì— ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”
3. ì•ˆì „í•œ ìžì„¸ë¥¼ ìœ ì§€í•˜ì„¸ìš”

---

"""


class MedicalRAG:
    """Medical RAG system with safety guardrails."""

    def __init__(self, storage_dir: Optional[Path] = None):
        self.storage_dir = storage_dir or STORAGE_DIR
        self.index = None
        self.query_engine = None
        self._load_index()

    def _load_index(self):
        """Load persisted index from storage."""
        if not self.storage_dir.exists():
            raise FileNotFoundError(
                f"Storage directory not found: {self.storage_dir}"
            )

        # Configure HuggingFace local embedding
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-large-en-v1.5",
        )

        # Configure LLM (gpt-4o-mini for cost-effective responses)
        Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0.1)

        storage_context = StorageContext.from_defaults(
            persist_dir=str(self.storage_dir)
        )
        self.index = load_index_from_storage(storage_context)

        # Create query engine with system prompt
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=SIMILARITY_TOP_K,
            system_prompt=MEDICAL_SYSTEM_PROMPT,
        )

    def _detect_emergency(self, query: str) -> bool:
        """Detect emergency keywords in query."""
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in EMERGENCY_KEYWORDS)

    def _format_sources(self, response) -> str:
        """Format source nodes for citation."""
        if not response.source_nodes:
            return ""

        sources = []
        for i, node in enumerate(response.source_nodes, 1):
            source_name = node.metadata.get('source', 'Unknown')
            score = node.score if hasattr(node, 'score') and node.score else 'N/A'
            sources.append(f"{i}. {source_name} (relevance: {score:.3f})" if isinstance(score, float) else f"{i}. {source_name}")

        return "\n".join(sources)

    def ask(self, query: str, use_rag: bool = True) -> str:
        """
        Process a query and return response with safety guardrails.

        Args:
            query: User's question
            use_rag: Whether to use RAG document retrieval (default True)

        Returns:
            Formatted response with sources and disclaimers
        """
        # Check for emergency
        is_emergency = self._detect_emergency(query)

        # Format response
        result_parts = []

        if is_emergency:
            result_parts.append(EMERGENCY_RESPONSE_PREFIX)

        if use_rag:
            # RAG ì‚¬ìš©: ë¬¸ì„œ ê²€ìƒ‰ + LLM
            if not self.query_engine:
                return "ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            response = self.query_engine.query(query)
            result_parts.append(str(response))

            # Add sources if available
            sources = self._format_sources(response)
            if sources:
                result_parts.append(f"\n\n**ì°¸ê³  ë¬¸ì„œ:**\n{sources}")
        else:
            # RAG ìŠ¤í‚µ: LLMë§Œ ì‚¬ìš© (í† í° ì ˆì•½)
            from openai import OpenAI
            import os

            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            simple_prompt = f"""{MEDICAL_SYSTEM_PROMPT}

ì‚¬ìš©ìž ì§ˆë¬¸:
{query}

ìœ„ ê±´ê°• ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": simple_prompt}],
                max_tokens=500,
                temperature=0.3,
            )

            result_parts.append(completion.choices[0].message.content)

        return "".join(result_parts)

    def get_retriever(self):
        """Get the underlying retriever for advanced use cases."""
        return self.index.as_retriever(similarity_top_k=SIMILARITY_TOP_K)


# Singleton instance for easy import
_rag_instance: Optional[MedicalRAG] = None


def get_rag() -> MedicalRAG:
    """Get or create RAG instance."""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = MedicalRAG()
    return _rag_instance


def ask(query: str) -> str:
    """Convenience function to ask a question."""
    return get_rag().ask(query)
